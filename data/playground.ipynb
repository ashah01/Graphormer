{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttnBias(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Compute attention bias for each head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        num_spatial\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.spatial_pos_encoder = torch.nn.Embedding(num_spatial, num_heads, padding_idx=0)\n",
    "\n",
    "        self.graph_token_virtual_distance = torch.nn.Embedding(1, num_heads)\n",
    "\n",
    "        self.spatial_pos_encoder.weight.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "        self.graph_token_virtual_distance.weight.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, batched_data):\n",
    "        attn_bias, spatial_pos, x = (\n",
    "            batched_data[\"attn_bias\"],\n",
    "            batched_data[\"spatial_pos\"],\n",
    "            batched_data[\"x\"],\n",
    "        )\n",
    "        # # in_degree, out_degree = batched_data.in_degree, batched_data.in_degree\n",
    "        # edge_input, attn_edge_type = (\n",
    "        #     batched_data[\"edge_input\"],\n",
    "        #     batched_data[\"attn_edge_type\"],\n",
    "        # )\n",
    "\n",
    "        n_graph, n_node = x.size()[:2]\n",
    "        graph_attn_bias = attn_bias.clone()\n",
    "        graph_attn_bias = graph_attn_bias.unsqueeze(1).repeat(\n",
    "            1, self.num_heads, 1, 1\n",
    "        )  # [n_graph, n_head, n_node+1, n_node+1]\n",
    "\n",
    "        # spatial pos\n",
    "        # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n",
    "        spatial_pos_bias = self.spatial_pos_encoder(spatial_pos).permute(0, 3, 1, 2)\n",
    "        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + spatial_pos_bias\n",
    "\n",
    "        # # reset spatial pos here\n",
    "        # t = self.graph_token_virtual_distance.weight.view(1, self.num_heads, 1)\n",
    "        # graph_attn_bias[:, :, 1:, 0] = graph_attn_bias[:, :, 1:, 0] + t\n",
    "        # graph_attn_bias[:, :, 0, :] = graph_attn_bias[:, :, 0, :] + t\n",
    "\n",
    "        # # edge feature\n",
    "        # if self.edge_type == \"multi_hop\":\n",
    "        #     spatial_pos_ = spatial_pos.clone()\n",
    "        #     spatial_pos_[spatial_pos_ == 0] = 1  # set pad to 1\n",
    "        #     # set 1 to 1, x > 1 to x - 1\n",
    "        #     spatial_pos_ = torch.where(spatial_pos_ > 1, spatial_pos_ - 1, spatial_pos_)\n",
    "        #     if self.multi_hop_max_dist > 0:\n",
    "        #         spatial_pos_ = spatial_pos_.clamp(0, self.multi_hop_max_dist)\n",
    "        #         edge_input = edge_input[:, :, :, : self.multi_hop_max_dist, :]\n",
    "        #     # [n_graph, n_node, n_node, max_dist, n_head]\n",
    "        #     edge_input = self.edge_encoder(edge_input).mean(-2)\n",
    "        #     max_dist = edge_input.size(-2)\n",
    "        #     edge_input_flat = edge_input.permute(3, 0, 1, 2, 4).reshape(\n",
    "        #         max_dist, -1, self.num_heads\n",
    "        #     )\n",
    "        #     edge_input_flat = torch.bmm(\n",
    "        #         edge_input_flat,\n",
    "        #         self.edge_dis_encoder.weight.reshape(\n",
    "        #             -1, self.num_heads, self.num_heads\n",
    "        #         )[:max_dist, :, :],\n",
    "        #     )\n",
    "        #     edge_input = edge_input_flat.reshape(\n",
    "        #         max_dist, n_graph, n_node, n_node, self.num_heads\n",
    "        #     ).permute(1, 2, 3, 0, 4)\n",
    "        #     edge_input = (\n",
    "        #         edge_input.sum(-2) / (spatial_pos_.float().unsqueeze(-1))\n",
    "        #     ).permute(0, 3, 1, 2)\n",
    "        # else:\n",
    "        #     # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n",
    "        #     edge_input = self.edge_encoder(attn_edge_type).mean(-2).permute(0, 3, 1, 2)\n",
    "\n",
    "        # graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + edge_input\n",
    "        # graph_attn_bias = graph_attn_bias + attn_bias.unsqueeze(1)  # reset\n",
    "\n",
    "        return graph_attn_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6d5e05947be6167eb267d35887c10d38bc3ffe327cbeea7e712fb8b80c18b36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Graphormer-qKJ3QKr1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
